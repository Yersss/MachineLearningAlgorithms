{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should perform this lab in Kaggle. The good news is that you can use the dataset already present in kaggle following this link:\n",
    "https://www.kaggle.com/nafisur/dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'test_set', 'training_set']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"dataset\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "### set a string variable 'wd' for working directory to '../input/dataset/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "290a62b7e97810f0f5afaa58917693ed323e2712"
   },
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### Part 1 - Building the CNN\n",
    "\n",
    "Importing the Keras libraries and packages:\n",
    "Sequential, Conv2D, MaxPooling2D, Flatten and Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "473b11511ebc2922b178476d3ff94b49beaaccd4"
   },
   "source": [
    "# Initialize the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution\n",
    "add a convolution layer with 32 units 3x3 shape. The input shape for the images is 64x64x3 and the activation layer 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3),  activation = 'relu', input_shape = (64, 64, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling\n",
    "add a pooling layer for Max Pooling with a pool size of 2 by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a second convolutional layer which should be similar to the first one except one thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3),  activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening\n",
    "add a Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Full connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two Dense layers with 128 and 1 units respectively. The first layer should cut off the negative values, whereas the second layer should return the classes in form of probabilities. Could you guess which activation functions are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "15c9509a2a5ec105d7c6dcf4c0d13911bca8ed68"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "531b0ef3cd64af6ce344029c93e1771d759ac6f0"
   },
   "source": [
    "### Compiling the CNN\n",
    "compile the network with 'adam' optimizer, binary_crossentropy as a loss and accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the images. The code is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "041573842d4af7ba81413d61f4d0e5f632d38fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(wd+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(wd+'/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "c272fea5df360f5699c6d579fcc6463133d3538c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "800/800 [==============================] - 104s 130ms/step - loss: 7.9519 - acc: 0.5012 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 104s 129ms/step - loss: 7.9980 - acc: 0.4983 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 101s 126ms/step - loss: 7.9513 - acc: 0.5013 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 104s 130ms/step - loss: 7.9843 - acc: 0.4992 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 104s 129ms/step - loss: 7.9706 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 105s 131ms/step - loss: 7.9787 - acc: 0.4995 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 100s 125ms/step - loss: 7.9643 - acc: 0.5004 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9787 - acc: 0.4995 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 7.9656 - acc: 0.5004 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 7.9687 - acc: 0.5002 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9893 - acc: 0.4989 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 95s 118ms/step - loss: 7.9681 - acc: 0.5002 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 95s 118ms/step - loss: 7.9500 - acc: 0.5013 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9836 - acc: 0.4992 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 7.9650 - acc: 0.5004 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9594 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9812 - acc: 0.4994 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 7.9762 - acc: 0.4997 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 96s 119ms/step - loss: 7.9562 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 7.9830 - acc: 0.4993 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 7.9687 - acc: 0.5002 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 99s 123ms/step - loss: 7.9693 - acc: 0.5001 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 99s 123ms/step - loss: 7.9774 - acc: 0.4996 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 7.9675 - acc: 0.5002 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 7.9731 - acc: 0.4999 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d82b90da0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 800,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a single prediction. The code is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "361421cfe668693bf91e2570326af8293587b004"
   },
   "outputs": [],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/sample/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "4affb1bb66b0456c36606ad7dc033b165d8409c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
